{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9fad9a0",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "This is a simple training script to try and learn Delta Tables Change Data Feed feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13ce1b5",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e282ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Required for Spark to find Python executable\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77651a9",
   "metadata": {},
   "source": [
    "# Start Spark App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2b92c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = None\n",
    "if spark:\n",
    "    spark.stop()\n",
    "\n",
    "spark = (SparkSession.builder \n",
    "    .appName(\"spark_delta\") \n",
    "    # These 3 lines configure Spark to work with Delta Tables\n",
    "    # If dependencies are not present in your claspath yet they will be automatically downloaded\n",
    "    # It is important to have an adequate Java environment. Having more than 1 version\n",
    "    # installed and env variables not properly set may lead to errors\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-spark_2.13:4.0.0\") \n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \n",
    "    # Set this to prevent Spark from creating a directory called spark-warehouse \n",
    "    # where date is persisted\n",
    "    .config(\"spark.sql.warehouse.dir\", \"../data/\") \n",
    "    .getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c693eb95",
   "metadata": {},
   "source": [
    "# Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65de45a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "netflix_df = spark.read.csv('../data/netflix_titles.csv',\n",
    "                            header=True,  # the first line of the file is a header\n",
    "                            multiLine=True,  # rows can have break lines\n",
    "                            # quote = Sets a single character used for escaping quoted values where the separator can be part of the value.\n",
    "                            quote='\"',\n",
    "                            # escape = Sets a single character used for escaping quotes inside an already quoted value.\n",
    "                            escape='\"'\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352d6daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "netflix_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c94e98",
   "metadata": {},
   "source": [
    "# Write to Delta Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7fa328",
   "metadata": {},
   "outputs": [],
   "source": [
    "netflix_delta_table_name = \"netflix_delta\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1135d44",
   "metadata": {},
   "source": [
    "## write_delta_table\n",
    "\n",
    "Helper method for creating and appending data into table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60dbd360",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_delta_table(mode: str):\n",
    "    (netflix_df.write\n",
    "     .format('delta')\n",
    "     .mode(mode)\n",
    "     .saveAsTable(f\"{netflix_delta_table_name}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0063c57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_delta_table('overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f54153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show TBLPROPERTIES so we can see that initially it doesn't have\n",
    "# CDF (Change Data Feed) enabled by default\n",
    "spark.sql(f'SHOW TBLPROPERTIES {netflix_delta_table_name}').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef40210",
   "metadata": {},
   "source": [
    "# Enable CDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac048c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\n",
    "    \"ALTER TABLE netflix_delta SET TBLPROPERTIES (delta.enableChangeDataFeed = true)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5af792",
   "metadata": {},
   "source": [
    "# Append data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e130709e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append the same data, just to create a new write version\n",
    "# so we can search the difference between versions\n",
    "write_delta_table('append')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161009ff",
   "metadata": {},
   "source": [
    "# Get latest version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89695307",
   "metadata": {},
   "outputs": [],
   "source": [
    "netflix_delta_table = DeltaTable.forName(spark, \"netflix_delta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05311d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_operations_df = (netflix_delta_table\n",
    "                       .history()\n",
    "                       .where(\"operation IN ('WRITE', 'RESTORE')\")\n",
    "                       )\n",
    "write_operations_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afdbf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_write_version = (write_operations_df\n",
    "                      .select(F.max('version').alias('last_version'))\n",
    "                      .collect()[0]['last_version'])\n",
    "last_write_version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2401231",
   "metadata": {},
   "source": [
    "# Read changes between version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a479d165",
   "metadata": {},
   "source": [
    "## Spark SQL way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52b2239",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_changes_df_sql = (spark\n",
    "                   .sql(f\"SELECT * FROM table_changes('{netflix_delta_table_name}',{last_write_version})\"))\n",
    "last_changes_df_sql.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e490844",
   "metadata": {},
   "source": [
    "## Delta Table way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16d3711",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_changes_df_dt = (spark.read.format(\"delta\")\n",
    "                      .option(\"readChangeFeed\", \"true\")\n",
    "                      .option(\"startingVersion\", last_write_version)\n",
    "                      #   .option(\"endingVersion\", 10)\n",
    "                      .table(netflix_delta_table_name))\n",
    "last_changes_df_dt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
