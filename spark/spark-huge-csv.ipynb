{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Imports",
   "id": "15f675886b3078a4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T23:58:43.667267Z",
     "start_time": "2025-07-07T23:58:43.471585Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import sys\n",
    "from glob import glob\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Required for Spark to find Python executable\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "# Required to use the correct Java version\n",
    "os.environ['JAVA_HOME'] = '/opt/homebrew/opt/openjdk@17'"
   ],
   "id": "cacd1dc8bb9a11c6",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Generate huge CSV",
   "id": "63a7bc86996e5f0c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-08T00:02:12.440786Z",
     "start_time": "2025-07-08T00:02:10.770221Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import csv\n",
    "import random\n",
    "\n",
    "def generate_test_csv(filename: str, num_rows=1000000):\n",
    "    print(f\"Generating {filename} with {num_rows:,} rows...\")\n",
    "\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "\n",
    "        # Write header\n",
    "        writer.writerow(['id', 'name', 'amount', 'category', 'date'])\n",
    "\n",
    "        # Generate random data\n",
    "        categories = ['Food', 'Entertainment', 'Shopping', 'Transportation', 'Utilities']\n",
    "\n",
    "        for i in range(1, num_rows + 1):\n",
    "            # Generate amounts with different distributions\n",
    "            if random.random() < 0.3:  # 30% chance of high amounts\n",
    "                amount = round(random.uniform(100, 1000), 2)\n",
    "            else:  # 70% chance of lower amounts\n",
    "                amount = round(random.uniform(1, 150), 2)\n",
    "\n",
    "            row = [\n",
    "                i,\n",
    "                f\"User_{i}\",\n",
    "                amount,\n",
    "                random.choice(categories),\n",
    "                f\"2025-07-{random.randint(1, 31):02d}\"\n",
    "            ]\n",
    "            writer.writerow(row)\n",
    "\n",
    "    print(f\"Generated {filename} successfully!\")\n",
    "\n",
    "file_path = '../data/huge_csv.csv'\n",
    "size = 1000000\n",
    "generate_test_csv(file_path, size)"
   ],
   "id": "eabf6fbfaf7dae08",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating ../data/huge_csv.csv with 1,000,000 rows...\n",
      "Generated ../data/huge_csv.csv successfully!\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Start Spark App",
   "id": "c124ed395d9c4eb4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T23:59:34.990635Z",
     "start_time": "2025-07-07T23:59:31.239432Z"
    }
   },
   "cell_type": "code",
   "source": "spark = SparkSession.builder.appName(\"huge-csv\").getOrCreate()",
   "id": "1b1dbb8fcc203305",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/07/07 20:59:32 WARN Utils: Your hostname, Alissons-MacBook-Pro.local, resolves to a loopback address: 127.0.0.1; using 192.168.1.21 instead (on interface en0)\n",
      "25/07/07 20:59:32 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/07/07 20:59:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Read CSV file",
   "id": "824e84ecb22899b8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-08T00:02:31.340554Z",
     "start_time": "2025-07-08T00:02:30.141934Z"
    }
   },
   "cell_type": "code",
   "source": "csv_df = spark.read.csv(file_path, header=True)",
   "id": "3a14448cd63cbf3f",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-08T00:09:57.681395Z",
     "start_time": "2025-07-08T00:09:57.670450Z"
    }
   },
   "cell_type": "code",
   "source": [
    "csv_df = csv_df.withColumns({\n",
    "    'amount': F.col('amount').cast('float'),\n",
    "    'date': F.col('date').cast('date')\n",
    "})"
   ],
   "id": "aa02fb616ee6caa",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-08T00:09:58.304716Z",
     "start_time": "2025-07-08T00:09:58.302004Z"
    }
   },
   "cell_type": "code",
   "source": "csv_df.printSchema()",
   "id": "ff4acf6b7447a20a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- amount: float (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Filter rows",
   "id": "f618b729650723ff"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-08T00:10:54.400150Z",
     "start_time": "2025-07-08T00:10:54.379257Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Filter rows where category = 'Food'\n",
    "csv_food = csv_df.where(F.col('category') == 'Food')"
   ],
   "id": "14cddcfab482fc63",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Aggregate filtered",
   "id": "45f8818f2f752edc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-08T00:19:50.933701Z",
     "start_time": "2025-07-08T00:19:50.561609Z"
    }
   },
   "cell_type": "code",
   "source": [
    "csv_food.groupby('category').agg(\n",
    "    F.count('*').alias('count'),\n",
    "    F.min('amount').alias('min_amount'),\n",
    "    F.max('amount').alias('max_amount'),\n",
    "    F.round(F.avg('amount'), 2).alias('avg_amount')\n",
    ").show()"
   ],
   "id": "9bfadaf7df6cd7d6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+----------+----------+----------+\n",
      "|category| count|min_amount|max_amount|avg_amount|\n",
      "+--------+------+----------+----------+----------+\n",
      "|    Food|199806|       1.0|    999.99|    217.67|\n",
      "+--------+------+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 46
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
