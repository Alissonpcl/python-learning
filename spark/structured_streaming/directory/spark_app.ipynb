{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Summary\n",
    "\n",
    "This notebook is a simples example of Spark Structured Streaming for reading new files created in Directory and printing data to console"
   ],
   "id": "a803b24b2d678191"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Imports",
   "id": "fa9b8251731defcb"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Start Spark App",
   "id": "7c77ee35ae7b66f2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "spark = (SparkSession.builder\n",
    "         .appName(\"spark_structured_streaming_directory\")\n",
    "         .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "         .config(\"spark.sql.warehouse.dir\", \"data/\")\n",
    "         .getOrCreate())"
   ],
   "id": "236adea31add7516",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Setup",
   "id": "f516977077c2b303"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Define and create directories",
   "id": "e2526b67fe3f9645"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "paths = {\n",
    "    \"source\": \"data/source\",\n",
    "    \"bronze\": \"data/delta/bronze\",\n",
    "    \"new_records_ckpt\": \"data/_checkpoints/new_records\",\n",
    "    \"stats_ckpt\": \"data/_checkpoints/stats\",\n",
    "}\n",
    "\n",
    "# Ensure the source path exists so it\n",
    "# doesn't fail when creating the readStream\n",
    "import os\n",
    "os.makedirs(paths['source'], exist_ok=True)"
   ],
   "id": "7dea996659192e5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Start ReadStream",
   "id": "68a568f35160021f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "streaming_df = spark.readStream \\\n",
    "    .format(\"csv\") \\\n",
    "    .schema(schema) \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .load(paths[\"source\"])"
   ],
   "id": "aac5f0d4be46913f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Write Streams",
   "id": "c4e4a54bc6c7578"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## new_records_query",
   "id": "2b1bfaf964b12e44"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# This writeStream will output to console\n",
    "# every time a new record is captured\n",
    "# A new record is captured when a new file is\n",
    "# created in the source directory\n",
    "new_records_query = streaming_df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"checkpointLocation\", paths['new_records_ckpt']) \\\n",
    "    .start()"
   ],
   "id": "b1960ee167a8135d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## stats_query",
   "id": "5cf4d1ef2a02d359"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "stats_query = streaming_df.agg(\n",
    "    F.count(\"*\").alias(\"count\"),\n",
    "    F.avg(\"age\").alias(\"avg_age\"),\n",
    ") \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"checkpointLocation\", paths['stats_ckpt']) \\\n",
    "    .start()"
   ],
   "id": "2781072e184540fa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## await termination",
   "id": "634bb7aa48a114d9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# This cell will execute indefinitely and the outputs of\n",
    "# the write streams will be printed here\n",
    "spark.streams.awaitAnyTermination()"
   ],
   "id": "a9d8508d4df6bdf2",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
