{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    from_json, col, to_timestamp, window, avg\n",
    ")\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, LongType"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "spark = (SparkSession.builder\n",
    "         .appName(\"spark_structured_streaming\")\n",
    "         # These 3 lines configure Spark to work with Delta Tables\n",
    "         # If dependencies are not present in your claspath yet they will be automatically downloaded\n",
    "         # It is important to have an adequate Java environment. Having more than 1 version\n",
    "         # installed and env variables not properly set may lead to errors\n",
    "         .config(\"spark.jars.packages\",\n",
    "                 \"io.delta:delta-spark_2.13:4.0.0,org.apache.spark:spark-sql-kafka-0-10_2.13:4.0.0\")\n",
    "         .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "         .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "         # Set this to prevent Spark from creating a directory called spark-warehouse\n",
    "         # where date is persisted\n",
    "         .config(\"spark.sql.warehouse.dir\", \"data/\")\n",
    "         .getOrCreate())"
   ],
   "id": "236adea31add7516",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "paths = {\n",
    "    \"bronze\": f\"data/delta/bronze_raw\",\n",
    "    \"silver_minavg\": f\"data/delta/silver_minute_avg\",\n",
    "    \"ckpt_bronze\": f\"data/_checkpoints/bronze_raw\",\n",
    "    \"ckpt_silver\": f\"data/_checkpoints/silver_minute_avg\",\n",
    "}\n",
    "\n",
    "kafka_bootstrap = \"127.0.0.1:9092\"\n",
    "topic = \"iot\"\n"
   ],
   "id": "7dea996659192e5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "raw_kafka = (spark.readStream.format(\"kafka\")\n",
    "             .option(\"kafka.bootstrap.servers\", kafka_bootstrap)\n",
    "             .option(\"subscribe\", topic)\n",
    "             .option(\"startingOffsets\", \"earliest\")\n",
    "             .load())"
   ],
   "id": "aac5f0d4be46913f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "json_df = raw_kafka.selectExpr(\n",
    "    \"CAST(value AS STRING) AS json\",\n",
    "    \"topic\", \"partition\", \"offset\",\n",
    "    \"timestamp as kafka_timestamp\"\n",
    ")"
   ],
   "id": "8a7bcffa24a22baa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "payload_schema = StructType([\n",
    "    StructField(\"ts\", LongType(), True),  # epoch millis from the device or gateway\n",
    "    StructField(\"device_id\", StringType(), True),\n",
    "    StructField(\"metric\", DoubleType(), True),\n",
    "    StructField(\"unit\", StringType(), True),\n",
    "])"
   ],
   "id": "e1d86d7cc3b3c329",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "parsed = json_df.select(\n",
    "    from_json(col(\"json\"), payload_schema).alias(\"d\"),\n",
    "    \"topic\", \"partition\", \"offset\", \"kafka_timestamp\"\n",
    ").select(\n",
    "    col(\"d.ts\").alias(\"event_ts_ms\"),\n",
    "    col(\"d.device_id\"),\n",
    "    col(\"d.metric\"),\n",
    "    col(\"d.unit\"),\n",
    "    col(\"topic\").alias(\"_kafka_topic\"),\n",
    "    col(\"partition\").alias(\"_kafka_partition\"),\n",
    "    col(\"offset\").alias(\"_kafka_offset\"),\n",
    "    col(\"kafka_timestamp\").alias(\"_kafka_timestamp\")\n",
    ").withColumn(\"event_time\", to_timestamp((col(\"event_ts_ms\") / 1000).cast(\"double\")))  # event-time in seconds\n"
   ],
   "id": "44af1a284811ce3f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "bronze_q = (parsed.writeStream\n",
    "            .format(\"delta\")\n",
    "            .option(\"checkpointLocation\", paths[\"ckpt_bronze\"])\n",
    "            .outputMode(\"append\")\n",
    "            .option(\"mergeSchema\", \"true\")\n",
    "            .start(paths[\"bronze\"]))"
   ],
   "id": "26796679c5a1b48f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "minute_avg = (parsed\n",
    ".withWatermark(\"event_time\", \"2 minutes\")  # tolerate late events up to 2 min\n",
    ".groupBy(\n",
    "    col(\"device_id\"),\n",
    "    window(col(\"event_time\"), \"1 minute\").alias(\"win\")\n",
    ")\n",
    ".agg(avg(\"metric\").alias(\"avg_metric\"))\n",
    ".select(\n",
    "    col(\"device_id\"),\n",
    "    col(\"win.start\").alias(\"window_start\"),\n",
    "    col(\"win.end\").alias(\"window_end\"),\n",
    "    col(\"avg_metric\")\n",
    ")\n",
    ")"
   ],
   "id": "6b56a033e630afbb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "silver_q = (minute_avg.writeStream\n",
    "            .format(\"delta\")\n",
    "            .option(\"checkpointLocation\", paths[\"ckpt_silver\"])\n",
    "            .outputMode(\"append\")  # with watermark + tumbling window, we can append final results\n",
    "            .option(\"mergeSchema\", \"true\")\n",
    "            .start(paths[\"silver_minavg\"]))"
   ],
   "id": "51858e3b77fab657",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(silver_q.isActive)\n",
    "silver_q.stop()\n",
    "# bronze_q.stop()\n",
    "\n",
    "# spark.streams.awaitAnyTermination(10)"
   ],
   "id": "1c4a774d474150f9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "spark.sql(f\"CREATE TABLE IF NOT EXISTS iot_bronze USING DELTA LOCATION '{paths['bronze'][5:]}'\")\n",
    "spark.sql(f\"CREATE TABLE IF NOT EXISTS iot_silver_minavg USING DELTA LOCATION '{paths['silver_minavg'][5:]}'\")"
   ],
   "id": "6b2df6772ea65ab3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "spark.sql(f\"describe extended iot_silver_minavg\").show(truncate=False)",
   "id": "3a586009859ae946",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "spark.sql(\"SELECT * FROM iot_silver_minavg\").show(truncate=False)",
   "id": "c440aa9a18c493a5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_bronze = spark.read.parquet(paths[\"bronze\"])\n",
    "df_silver = spark.read.parquet(paths[\"silver_minavg\"])"
   ],
   "id": "769c0116610da38b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_bronze.count()",
   "id": "7522dd87ec87adc9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_silver.show()",
   "id": "fac0dfbcbaeaa878",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
